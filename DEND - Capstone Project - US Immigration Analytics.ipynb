{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics on Tourists visiting US\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "An end-to-end Data Pipeline to ingest, transform and load the data related for doing analytics on US immigration.\n",
    "\n",
    "The project follows the following steps:\n",
    "* Step 1: Scope of the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and  Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The scope of this project is to enable the Data Analysts to do analytics on US immigration. This project focuses on ingesting, cleaning, transforming and storing data related to US immigration, so that the data analysts can derive intelligensce from that data. Also, data quality checks on the data will be performed.\n",
    "\n",
    "US I94 Immigrartion data and US Demographics data will be used for this purpose along with some lookup data like ISO cuntry code data. The final data will be cleaned I94 data and aggregated US demographics data where the both tables can be joined by the US state names.\n",
    "\n",
    "Apache Spark will be used for cleaning and transforming purposes. The final data will be stored in partitioned Parquet format.\n",
    "\n",
    "Data Analysts can use the output of this project to get answers for some questions like:\n",
    "1. Based on Age group which state they prefer\n",
    "2. Age Group and Visa Type and Origin country\n",
    "3. Which state receive more people\n",
    "4. Is the total number of airports in a state affect immigration in that state\n",
    "etc.\n",
    "\n",
    "#### Data \n",
    "Below are the list of datasets used: \n",
    "##### 1. I94 Immigration data 2016:\n",
    "Source: https://travel.trade.gov/research/reports/i94/historical/2016.html <br />\n",
    "Format: SAS format <br />\n",
    "This data comes from the US National Tourism and Trade Office. Each report contains international visitor arrival statistics by world regions and select countries (including top 20), type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry (for select countries).\n",
    "\n",
    "##### 2. US Cities Demographics:\n",
    "Source: https://www.ip2location.com/free/iso3166-2 <br />\n",
    "Format: CSV Text format (separator is \";\") <br />\n",
    "This data comes from the US Census Bureau's 2015 American Community Survey. It contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data is impored from OpenSoft. \n",
    "\n",
    "##### 3. ISO 3166-2 Subdivision Codes:\n",
    "Source: https://www.ip2location.com/free/iso3166-2 <br />\n",
    "Format: CSV Text format (separator is \";\") <br />\n",
    "IP2Locationâ„¢ ISO 3166-2 Subdivision Code is a free data offered for your download. This data contains the ISO3166-2 code for the states/regions used in our geolocation database. You can easily retrieve the ISO3166-2 code by mapping the country code and subdivision name.\n",
    "\n",
    "##### 4. US Airport codes:\n",
    "Source: https://datahub.io/core/airport-codes <br />\n",
    "Format: CSV Text format (separator is \",\") <br />\n",
    "This is a simple table of airport codes and corresponding cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf,when,length\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder \\\n",
    "    .appName(\"DEND - US Immigration Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ ALL DATASETS\n",
    "\n",
    "df_i94  = spark.read.parquet('data/i94_2016_data_parquet/')\n",
    "df_demo = spark.read.csv('data/us_cities_demographics.csv', sep=\";\", header=True)\n",
    "df_loc  = spark.read.csv('data/IP2LOCATION_ISO3166.csv', sep=\",\", header=True)\n",
    "df_air  = spark.read.csv('data/airport_codes_csv.csv', sep=\",\", header=True)\n",
    "\n",
    "df_port  = spark.read.csv('data/i94port_codes.csv', sep=\",\", header=True)\n",
    "df_country = spark.read.csv('data/i94cntyl_codes.csv', sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATASET 1:   I94 Immigration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE ClEANING: \n",
      "+------+------+-------+-------+------+-------+--------------------+------+--------------+---------+\n",
      "| i94yr|i94mon|i94addr|biryear|gender|i94port|           port_name|i94res|origin_country|visa_type|\n",
      "+------+------+-------+-------+------+-------+--------------------+------+--------------+---------+\n",
      "|2016.0|   4.0|   null| 1977.0|     M|    LOS|LOS ANGELES, CA  ...|   504|        PANAMA| Pleasure|\n",
      "|2016.0|   4.0|   null| 1985.0|     F|    TOR|TORONTO, CANADA  ...|   249|          IRAN| Pleasure|\n",
      "|2016.0|   4.0|   null| 1963.0|     M|    NYC|NEW YORK, NY     ...|   249|          IRAN| Pleasure|\n",
      "|2016.0|   4.0|   null| 1945.0|  null|    NYC|NEW YORK, NY     ...|   251|        ISRAEL| Pleasure|\n",
      "|2016.0|   4.0|   null| 1950.0|     M|    NAS|NASSAU, BAHAMAS  ...|   251|        ISRAEL| Pleasure|\n",
      "|2016.0|   4.0|   null| 1980.0|     F|    ATL|ATLANTA, GA      ...|   251|        ISRAEL| Pleasure|\n",
      "|2016.0|   4.0|   null| 1990.0|     F|    NEW|NEWARK/TETERBORO,...|   251|        ISRAEL| Pleasure|\n",
      "|2016.0|   4.0|   null| 1956.0|     F|    NEW|NEWARK/TETERBORO,...|   251|        ISRAEL| Pleasure|\n",
      "|2016.0|   4.0|   null| 1958.0|     F|    NEW|NEWARK/TETERBORO,...|   251|        ISRAEL| Pleasure|\n",
      "|2016.0|   4.0|   null| 1958.0|     M|    NEW|NEWARK/TETERBORO,...|   251|        ISRAEL| Pleasure|\n",
      "+------+------+-------+-------+------+-------+--------------------+------+--------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "ClEANING: After replacing null i94addr with dest_state: \n",
      "+------+------+-------+------+-------+--------------------+------+--------------+---------+----------+\n",
      "| i94yr|i94mon|biryear|gender|i94port|           port_name|i94res|origin_country|visa_type|dest_state|\n",
      "+------+------+-------+------+-------+--------------------+------+--------------+---------+----------+\n",
      "|2016.0|   4.0| 1977.0|     M|    LOS|LOS ANGELES, CA  ...|   504|        PANAMA| Pleasure|        CA|\n",
      "|2016.0|   4.0| 1985.0|     F|    TOR|TORONTO, CANADA  ...|   249|          IRAN| Pleasure|    CANADA|\n",
      "|2016.0|   4.0| 1963.0|     M|    NYC|NEW YORK, NY     ...|   249|          IRAN| Pleasure|        NY|\n",
      "|2016.0|   4.0| 1945.0|  null|    NYC|NEW YORK, NY     ...|   251|        ISRAEL| Pleasure|        NY|\n",
      "|2016.0|   4.0| 1950.0|     M|    NAS|NASSAU, BAHAMAS  ...|   251|        ISRAEL| Pleasure|   BAHAMAS|\n",
      "|2016.0|   4.0| 1980.0|     F|    ATL|ATLANTA, GA      ...|   251|        ISRAEL| Pleasure|        GA|\n",
      "|2016.0|   4.0| 1990.0|     F|    NEW|NEWARK/TETERBORO,...|   251|        ISRAEL| Pleasure|        NJ|\n",
      "|2016.0|   4.0| 1956.0|     F|    NEW|NEWARK/TETERBORO,...|   251|        ISRAEL| Pleasure|        NJ|\n",
      "|2016.0|   4.0| 1958.0|     F|    NEW|NEWARK/TETERBORO,...|   251|        ISRAEL| Pleasure|        NJ|\n",
      "|2016.0|   4.0| 1958.0|     M|    NEW|NEWARK/TETERBORO,...|   251|        ISRAEL| Pleasure|        NJ|\n",
      "+------+------+-------+------+-------+--------------------+------+--------------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "ClEANING: After removing all non-US states: \n",
      "+------+------+-------+------+-------+--------------------+------+--------------+---------+----------+\n",
      "| i94yr|i94mon|biryear|gender|i94port|           port_name|i94res|origin_country|visa_type|dest_state|\n",
      "+------+------+-------+------+-------+--------------------+------+--------------+---------+----------+\n",
      "|2016.0|   4.0| 1976.0|     F|    LOS|LOS ANGELES, CA  ...|   438|     AUSTRALIA| Business|        CA|\n",
      "|2016.0|   4.0| 1984.0|     F|    LOS|LOS ANGELES, CA  ...|   438|     AUSTRALIA| Business|        NV|\n",
      "|2016.0|   4.0| 1987.0|     M|    LOS|LOS ANGELES, CA  ...|   438|     AUSTRALIA| Business|        WA|\n",
      "|2016.0|   4.0| 1987.0|     F|    LOS|LOS ANGELES, CA  ...|   438|     AUSTRALIA| Business|        WA|\n",
      "|2016.0|   4.0| 1988.0|     M|    LOS|LOS ANGELES, CA  ...|   438|     AUSTRALIA| Business|        WA|\n",
      "|2016.0|   4.0| 1959.0|     M|    HHW|HONOLULU, HI     ...|   464|   NEW ZEALAND| Pleasure|        HI|\n",
      "|2016.0|   4.0| 1950.0|     F|    HHW|HONOLULU, HI     ...|   464|   NEW ZEALAND| Pleasure|        HI|\n",
      "|2016.0|   4.0| 1975.0|     F|    HHW|HONOLULU, HI     ...|   464|   NEW ZEALAND| Pleasure|        HI|\n",
      "|2016.0|   4.0| 1989.0|     M|    HOU|HOUSTON, TX      ...|   464|   NEW ZEALAND| Pleasure|        FL|\n",
      "|2016.0|   4.0| 1990.0|     F|    LOS|LOS ANGELES, CA  ...|   464|   NEW ZEALAND| Pleasure|        CA|\n",
      "+------+------+-------+------+-------+--------------------+------+--------------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- TRASFORMATIONS ---\n",
    "\n",
    "# Select only the necessary columns from original data\n",
    "df_i94_select = df_i94.select(\"i94yr\", \"i94mon\", \"i94port\", \"i94addr\", \"i94visa\", \"i94res\", \"biryear\", \"gender\") \n",
    "\n",
    "# i94port -> US Port Code. Replace with Port name\n",
    "# i94res -> Origin country code. Replace with country name\n",
    "# i94visa -> Visa codes collapsed into three categories [1 = Business, 2 = Pleasure, 3 = Student]\n",
    "   \n",
    "df_i94_transform = df_i94_select \\\n",
    "                    .join(df_port, df_i94.i94port == df_port.i94port, \"left\") \\\n",
    "                    .join(df_country, df_i94.i94res == df_country.i94res, \"left\") \\\n",
    "                    .withColumn(\"visa_type\", when(df_i94_select.i94visa == 1.0, 'Business') \\\n",
    "                                            .when(df_i94_select.i94visa == 2.0, 'Pleasure') \\\n",
    "                                            .otherwise('Student')) \\\n",
    "                    .drop(df_i94_select.i94visa) \\\n",
    "                    .drop(df_i94_select.i94res) \\\n",
    "                    .drop(df_i94_select.i94port) \\\n",
    "                    .withColumnRenamed(\"description\", \"port_name\") \\\n",
    "                    .withColumnRenamed(\"country\", \"origin_country\") \n",
    "\n",
    "# --- CLEANING ---\n",
    "\n",
    "# 1. Some i94addr are NULL even though i94port is Not NULL\n",
    "\n",
    "print(\"BEFORE ClEANING: \")\n",
    "df_i94_transform \\\n",
    "    .filter(\"i94addr is null\") \\\n",
    "    .show(10)\n",
    "\n",
    "# Replace i94addr null values using port_name column\n",
    "def get_state(i94addr, port_name):\n",
    "    if i94addr is not None:\n",
    "        return i94addr\n",
    "    else:\n",
    "        split = port_name.split(',')\n",
    "        if split[1] is not None: return split[1].strip()\n",
    "        else: return  split[0].strip()\n",
    "\n",
    "get_state = udf(get_state, StringType())\n",
    "\n",
    "df_i94_transform = df_i94_transform \\\n",
    "                        .withColumn(\"dest_state\", get_state(df_i94_transform.i94addr, df_i94_transform.port_name)) \\\n",
    "                        .drop(df_i94_select.i94addr)\n",
    "\n",
    "print(\"ClEANING: After replacing null i94addr with dest_state: \")\n",
    "df_i94_transform.filter(\"i94addr is null\").show(10)\n",
    "\n",
    "\n",
    "# 2. Keep only US states and remove all other non-US places\n",
    "\n",
    "df_i94_transform = df_i94_transform \\\n",
    "                        .filter(length(df_i94_transform.dest_state) == 2)\n",
    "\n",
    "print(\"ClEANING: After removing all non-US states: \")\n",
    "df_i94_transform.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DATASET 2:   US Demographics Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE AGGREGATION for Texas:\n",
      "+--------------+----------+--------------------+------+----------+--------+----------+---------+-------------+\n",
      "|          City|state_code|                Race| count|median_age|pop_male|pop_female|pop_total|no_immigrants|\n",
      "+--------------+----------+--------------------+------+----------+--------+----------+---------+-------------+\n",
      "|        Laredo|        TX|American Indian a...|  1253|      28.8|  124305|    131484|   255789|        68427|\n",
      "|  Flower Mound|        TX|  Hispanic or Latino|  6149|      40.2|   35200|     35824|    71024|         6860|\n",
      "|Corpus Christi|        TX|               White|292663|      35.0|  160488|    163594|   324082|        30834|\n",
      "|         Bryan|        TX|Black or African-...| 11914|      29.4|   41761|     40345|    82106|        12014|\n",
      "|       Killeen|        TX|American Indian a...|  2362|      29.2|   69442|     71367|   140809|        15769|\n",
      "|       El Paso|        TX|American Indian a...|  7359|      33.1|  332797|    348339|   681136|       159709|\n",
      "|    Richardson|        TX|Black or African-...| 13256|      35.5|   54676|     56151|   110827|        29579|\n",
      "|          Waco|        TX|Black or African-...| 29883|      29.3|   63452|     68890|   132342|        14235|\n",
      "|          Waco|        TX|               Asian|  4230|      29.3|   63452|     68890|   132342|        14235|\n",
      "|       Garland|        TX|  Hispanic or Latino| 90989|      34.5|  116406|    120430|   236836|        62975|\n",
      "|       Garland|        TX|               Asian| 27217|      34.5|  116406|    120430|   236836|        62975|\n",
      "| Wichita Falls|        TX|American Indian a...|  1503|      34.0|   55775|     48934|   104709|         9855|\n",
      "|      Amarillo|        TX|               White|174214|      33.8|   99391|    100260|   199651|        21124|\n",
      "|       El Paso|        TX|               Asian| 12370|      33.1|  332797|    348339|   681136|       159709|\n",
      "|    Fort Worth|        TX|               White|575180|      32.6|  414126|    422843|   836969|       143404|\n",
      "|    Fort Worth|        TX|American Indian a...|  7504|      32.6|  414126|    422843|   836969|       143404|\n",
      "|      Beaumont|        TX|American Indian a...|   908|      31.6|   57326|     60784|   118110|        10516|\n",
      "| Grand Prairie|        TX|Black or African-...| 54934|      33.1|   90811|     96944|   187755|        44888|\n",
      "|        Laredo|        TX|               White|246442|      28.8|  124305|    131484|   255789|        68427|\n",
      "|      Mesquite|        TX|               White| 98597|      34.6|   69240|     75884|   145124|        26755|\n",
      "+--------------+----------+--------------------+------+----------+--------+----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "AFTER AGGREGATION for Texas:\n",
      "+----------+--------------------+--------+----------+---------+-------------+-----------------+\n",
      "|state_code|                Race|pop_male|pop_female|pop_total|no_immigrants|       median_age|\n",
      "+----------+--------------------+--------+----------+---------+-------------+-----------------+\n",
      "|        TX|  Hispanic or Latino| 7063571|   7236412| 14299983|      2942164| 33.3403507366515|\n",
      "|        TX|American Indian a...| 6755508|   6908525| 13664033|      2804498|33.26938757604482|\n",
      "|        TX|Black or African-...| 6951215|   7115191| 14066406|      2892596|33.48333316379123|\n",
      "|        TX|               Asian| 7028329|   7195119| 14223448|      2916632|33.45535700661795|\n",
      "|        TX|               White| 7063571|   7236412| 14299983|      2942164| 33.3403507366515|\n",
      "+----------+--------------------+--------+----------+---------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- TRASFORMATIONS ---\n",
    "\n",
    "# Select only the necessary columns from original data\n",
    "df_demo_select = df_demo \\\n",
    "                    .drop(\"State\") \\\n",
    "                    .drop(\"Average Household Size\") \\\n",
    "                    .drop(\"Number of Veterans\") \\\n",
    "                    .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "                    .withColumn(\"median_age\", df_demo[\"Median Age\"].cast('float')) \\\n",
    "                    .withColumn(\"pop_male\", df_demo[\"Male Population\"].cast('int')) \\\n",
    "                    .withColumn(\"pop_female\", df_demo[\"Female Population\"].cast('int')) \\\n",
    "                    .withColumn(\"pop_total\", df_demo[\"Total Population\"].cast('int')) \\\n",
    "                    .withColumn(\"no_immigrants\", df_demo[\"Foreign-born\"].cast('int')) \\\n",
    "                    .withColumn(\"count\", df_demo[\"Count\"].cast('int')) \\\n",
    "                    .drop(\"Median Age\") \\\n",
    "                    .drop(\"Male Population\") \\\n",
    "                    .drop(\"Female Population\") \\\n",
    "                    .drop(\"Total Population\") \\\n",
    "                    .drop(\"Foreign-born\")\n",
    "\n",
    "# --- CLEANING ---\n",
    "\n",
    "# 1. Aggregate the data by US State Name and Race\n",
    "\n",
    "print(\"BEFORE AGGREGATION for Texas:\")\n",
    "df_demo_select.filter(\"state_code == 'TX'\").show()\n",
    "\n",
    "df_demo_transform = df_demo_select \\\n",
    "                            .drop(\"City\") \\\n",
    "                            .groupBy(\"state_code\", \"Race\") \\\n",
    "                            .agg({'median_age': 'mean',\n",
    "                                  'pop_male':'sum', \n",
    "                                  'pop_female':'sum', \n",
    "                                  'pop_total':'sum', \n",
    "                                  'no_immigrants':'sum'}) \\\n",
    "                            .withColumnRenamed(\"avg(median_age)\", \"median_age\") \\\n",
    "                            .withColumnRenamed(\"sum(pop_male)\", \"pop_male\") \\\n",
    "                            .withColumnRenamed(\"sum(pop_female)\", \"pop_female\") \\\n",
    "                            .withColumnRenamed(\"sum(pop_total)\", \"pop_total\") \\\n",
    "                            .withColumnRenamed(\"sum(no_immigrants)\", \"no_immigrants\")\n",
    "        \n",
    "print(\"AFTER AGGREGATION for Texas:\")\n",
    "df_demo_transform.filter(\"state_code == 'TX'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DATASET 3:   ISO Subdivision Codes Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----+\n",
      "|country_code|    subdivision_name| code|\n",
      "+------------+--------------------+-----+\n",
      "|          US|             Alabama|US-AL|\n",
      "|          US|              Alaska|US-AK|\n",
      "|          US|             Arizona|US-AZ|\n",
      "|          US|            Arkansas|US-AR|\n",
      "|          US|          California|US-CA|\n",
      "|          US|            Colorado|US-CO|\n",
      "|          US|         Connecticut|US-CT|\n",
      "|          US|            Delaware|US-DE|\n",
      "|          US|District of Columbia|US-DC|\n",
      "|          US|             Florida|US-FL|\n",
      "+------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- TRASFORMATION ---\n",
    "\n",
    "# From this ISO lookup table filter only US state codes\n",
    "\n",
    "df_loc \\\n",
    "    .filter(\"country_code = 'US'\") \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DATASET 4:   US airport Code Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|iso_region|count|\n",
      "+----------+-----+\n",
      "|     US-TN|  228|\n",
      "|     US-OK|  372|\n",
      "|     US-VT|   66|\n",
      "|     US-SD|  162|\n",
      "|     US-WA|  382|\n",
      "|     US-IN|  487|\n",
      "|     US-AL|  198|\n",
      "|     US-NY|  404|\n",
      "|     US-MS|  212|\n",
      "|     US-NC|  349|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- TRASFORMATION ---\n",
    "\n",
    "# Select only the necessary columns from original data\n",
    "df_air_select = df_air.select(\"type\", \"iso_country\", \"iso_region\")\n",
    "\n",
    "# Aggregare total number of airports in each US state\n",
    "df_air_transform = df_air_select \\\n",
    "                        .filter(\"iso_country == 'US'\") \\\n",
    "                        .filter(\"type like '%airport%'\") \\\n",
    "                        .groupBy(\"iso_region\") \\\n",
    "                        .count()\n",
    "\n",
    "df_air_transform.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "Entities:\n",
    "- i94_table\n",
    "- us_demo_table\n",
    "- airport_count_table\n",
    "\n",
    "The above 3 tables can be joined with the state_name key. With above entities, will answer the below questions for data analysts:\n",
    "1. Based on Age group which state they prefer\n",
    "2. Age Group and Visa Type and Origin country\n",
    "3. Which state receive more people\n",
    "4. Is the total number of airports in a state affect immigration in that state\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "* Ingest all data\n",
    "    * Ingest i94 dataset\n",
    "    * Ingest US demographics dataset\n",
    "    * Ingest ISO location code lookup dataset\n",
    "    * Ingest Airport code lookup dataset\n",
    "    \n",
    "\n",
    "* Cleansing and Transformation\n",
    "    * Select only the columns that are needed for further analysis\n",
    "    * Use lookup tables to map codes with description\n",
    "    \n",
    "* Joining tables\n",
    "    * Key value: state_name\n",
    "    * Build the final data model by joining tables with key values to get denormalised form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Check 1: Row Count is non-zero\n",
    "\n",
    "\n",
    "# Data Quality Check 2: No Duplicate rows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* i94_table:\n",
    "    * i94yr -> 4 digit Year \n",
    "    * i94mon -> Numeric Month\n",
    "    * biryear -> Birth year of immigrant\n",
    "    * gender -> Gender ['M', 'F', Null]\n",
    "    * i94port -> Arrival Port code\n",
    "    * port_name -> Arrival Port name\n",
    "    * origin_country -> Origin country of immigrant\n",
    "    * visa_type -> VISA Type [Business, Pleasure, Student]\n",
    "    * dest_state -> Destination State in US\n",
    "\n",
    "* US_demographics_table\n",
    "   * state_code -> State Code\n",
    "   * Race -> Race of people [Black, Asian, White, Hispanic]\n",
    "   * pop_male -> Total Male Population\n",
    "   * pop_female -> Total Female Population\n",
    "   * pop_total -> Total Population\n",
    "   * no_immigrants -> Total number of Immigrants\n",
    "   * median_age -> Median Age\n",
    "   \n",
    "   \n",
    "* Airport_count_table\n",
    "   * iso_region -> ISO US State code\n",
    "   * count -> Total number of airports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Project Write Up\n",
    "* Rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "  - Computation Tool: Apache Spark\n",
    "  - Data Storage: Currently the data resides in local system (single node). But as the data grows we can store the data to any distributed storage partitioned by year like Amazon S3 or HDFS for fast parallel processing using Apache Spark. \n",
    "\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "  - Ideally each day the number of immigrants coming into US will be less. So it makes sense to update the Immigration data every week. Say, for example, every Sunday at midnight. \n",
    "\n",
    "\n",
    "* Approach for handling the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "    - Soln: Distributed storage systems like Amazon S3 or HDFS can handle more data by adding more nodes and storing the data in distributed manner.\n",
    "    \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "    - Soln: Schedule the ETL job using daily using any scheduler like Apache Airflow or Apache Oozie daily at 6.30 AM so that the data get refreshed on the dashboard by 7 AM daily.\n",
    "    \n",
    " * The database needed to be accessed by 100+ people.\n",
    "   - Soln: Create an Access group with READ access for the database and add the members who require access to the database to this group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
